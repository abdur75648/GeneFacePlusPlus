base_config:
  - egs/egs_bases/radnerf/lm3d_radnerf.yaml

video_id: May # the video file should be located at `data/raw/videos/<video_id>.mp4`
amp: true

nerf_keypoint_mode: lm68 # lm68 | lm131 | lm468

cond_out_dim: 64
cond_dropout_rate: 0.

individual_embedding_dim: 4 # 32
hidden_dim_sigma: 128 # 64 by radnerf is too small
geo_feat_dim: 128 # 64 by radnerf is too small
num_layers_color: 2 # 2
hidden_dim_color: 128 # 64 by radnerf is too small
num_layers_ambient: 3 # 3
hidden_dim_ambient: 128 # 64 by radnerf is too small
lambda_ambient: 1. 
polygon_face_mask: true

n_rays: 65536 # 262144 # 65536 # num rays sampled per image for each training step, default 256*256

clip_grad_norm: 0. # 1. in fp16 leads to nan

# to tune scale
# https://github.com/NVlabs/instant-ngp/blob/master/docs/nerf_dataset_tips.md
# https://github.com/ashawkey/torch-ngp/issues/112
# The occupancy grid works fine in LEGO dataset. (~10x accelerated)
# In my experiment (and on my dataset), I found that occupancy grid sampling is vulnerable to scale.
# In specific scale range, the occ grid sampling works and accelerates rendering.
# But outside of that range, the acceleration gain disappears, or it fails to converge at all.
# (Without the occ grid sampling, the model has learned the scene in that scales.)
# I think this is reasonable because covering the the camera-viewed region with a predefined grid 
# is easier to fail than sampling without grids.
# With an manual scale tuning, I can get the expected acceleration gain.